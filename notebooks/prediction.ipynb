{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from pointcept.models import build_model\n",
    "from pointcept.utils.visualization import save_point_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = dict(\n",
    "    type=\"DefaultSegmentor\",\n",
    "    backbone=dict(\n",
    "        type=\"PT-v2m2\",\n",
    "        in_channels=4,\n",
    "        num_classes=8,\n",
    "        patch_embed_depth=1,\n",
    "        patch_embed_channels=48,\n",
    "        patch_embed_groups=6,\n",
    "        patch_embed_neighbours=8,\n",
    "        enc_depths=(2, 2, 6, 2),\n",
    "        enc_channels=(96, 192, 384, 512),\n",
    "        enc_groups=(12, 24, 48, 64),\n",
    "        enc_neighbours=(16, 16, 16, 16),\n",
    "        dec_depths=(1, 1, 1, 1),\n",
    "        dec_channels=(48, 96, 192, 384),\n",
    "        dec_groups=(6, 12, 24, 48),\n",
    "        dec_neighbours=(16, 16, 16, 16),\n",
    "        grid_sizes=(0.15, 0.375, 0.9375, 2.34375),  # x3, x2.5, x2.5, x2.5\n",
    "        attn_qkv_bias=True,\n",
    "        pe_multiplier=False,\n",
    "        pe_bias=True,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.3,\n",
    "        enable_checkpoint=False,\n",
    "        unpool_backend=\"map\",  # map / interp\n",
    "    ),\n",
    "    criteria=[\n",
    "        dict(type=\"CrossEntropyLoss\", loss_weight=1.0, ignore_index=-1),\n",
    "        dict(type=\"LovaszLoss\", mode=\"multiclass\", loss_weight=1.0, ignore_index=-1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_learning_map(ignore_index):\n",
    "    learning_map = {\n",
    "        ignore_index: ignore_index,\n",
    "        0: 2,  # \"Two-wheel Vehicle\",\n",
    "        1: 3,  # \"Pedestrian\"\n",
    "        3: 0,  # \"Car\"\n",
    "        8: 1,  # \"Truck/Bus\"\n",
    "        10: 7,  # \"Traffic Light\"\n",
    "        12: 6,  # \"Traffic Sign\"\n",
    "        40: 4,  # \"Road\"\n",
    "        48: 5,  # \"Sidewalk\"\n",
    "    }\n",
    "    return learning_map\n",
    "ignore_index = -1\n",
    "learning_map = get_learning_map(ignore_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(cfg_model).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all weights from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./exp/nia_cycle1/semseg-pt-v2m2-1-ft-split-val-yh/model/model_last.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_dir)\n",
    "state_dict = checkpoint[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict의 key에 \"module.\"이 추가되어 있는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "for name, value in state_dict.items():\n",
    "    if name.startswith(\"module.\"):\n",
    "        name = name[7:]\n",
    "    new_state_dict[name] = value\n",
    "state_dict = new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load point cloud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"/datasets/nia\")\n",
    "lidar_dataset_path_to_predict = data_root / \"collections/230822/230822_162106_K/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        label = json.load(f)\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_label_path_from_lidar_path(lidar_path):\n",
    "    collections_path = Path(data_root) / 'collections'\n",
    "    annotations_path = Path(data_root) / 'annotations'\n",
    "    return annotations_path / lidar_path.relative_to(collections_path).with_suffix('.json')\n",
    "\n",
    "\n",
    "def get_data_list(lidar_dataset_path):\n",
    "    collection_path = Path(lidar_dataset_path)\n",
    "    lidar_paths = sorted(collection_path.rglob('*.pcd'))\n",
    "    label_paths = list(map(get_label_path_from_lidar_path, lidar_paths))\n",
    "\n",
    "    return list(zip(\n",
    "        map(str, lidar_paths),\n",
    "        map(str, label_paths),\n",
    "    ))\n",
    "\n",
    "data_list = get_data_list(lidar_dataset_path_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_points(cloud1, cloud2):\n",
    "    # Create a cKDTree object for each point cloud\n",
    "    tree1 = cKDTree(cloud1)\n",
    "    tree2 = cKDTree(cloud2)\n",
    "    \n",
    "    # Fast search indices of common points of two point clouds\n",
    "    # ex) indices: [[tree2_index_i], [tree2_index_j], [], ...]\n",
    "    # len(indices) == len(tree1)\n",
    "    common_point_indices = tree1.query_ball_tree(tree2, r=0)\n",
    "    \n",
    "    # overlapping_indices = []\n",
    "    # for i, indices in enumerate(common_point_indices):\n",
    "    #     for j in indices:\n",
    "    #         overlapping_indices.append((i, j))\n",
    "    \n",
    "    return common_point_indices\n",
    "\n",
    "\n",
    "def create_segment_array(label_path, cloud_points):\n",
    "    label = read_json(label_path)\n",
    "\n",
    "    segment = np.ones((cloud_points.shape[0],), dtype=int) * ignore_index\n",
    "\n",
    "    annotations = label['annotations']\n",
    "    for annotation in annotations:\n",
    "        instance_points = annotation['3D_points']\n",
    "        instance_class_id = annotation['class_id']\n",
    "\n",
    "        indices = find_common_points(instance_points, cloud_points)\n",
    "        indices = np.array(indices, dtype=int).reshape(-1)\n",
    "        segment[indices] = instance_class_id\n",
    "    \n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(idx):\n",
    "    lidar_path, label_path = data_list[idx % len(data_list)]\n",
    "\n",
    "    # Read lidar\n",
    "    cloud = o3d.t.io.read_point_cloud(lidar_path)\n",
    "    coord = cloud.point['positions'].numpy()\n",
    "    strength = cloud.point['reflectivity'].numpy() / 255\n",
    "\n",
    "    # Read label\n",
    "    segment = create_segment_array(label_path, coord)\n",
    "    segment = np.vectorize(learning_map.__getitem__)(segment).astype(\n",
    "        np.int64\n",
    "    )\n",
    "\n",
    "    data_dict = dict(coord=coord, strength=strength, segment=segment)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_data(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data before feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data_dict):\n",
    "    data = dict()\n",
    "\n",
    "    # Transform: ToTensor, Collect\n",
    "    # ToTensor\n",
    "    data[\"coord\"] = torch.from_numpy(data_dict[\"coord\"]).float().cuda()\n",
    "    data[\"strength\"] = torch.from_numpy(data_dict[\"strength\"]).float().cuda()\n",
    "    # data[\"segment\"] = torch.from_numpy(data_dict[\"segment\"]).long().cuda()\n",
    "\n",
    "    # Collect\n",
    "    data[\"coord\"] = data[\"coord\"]\n",
    "    # data[\"segment\"] = data[\"segment\"]\n",
    "    data[\"offset\"] = torch.tensor([data[\"coord\"].shape[0]]).cuda()\n",
    "    data[\"feat\"] = torch.cat([data[key].float() for key in [\"coord\", \"strength\"]], dim=1).cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = transform(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_dict = model(data_dict)\n",
    "    logits = output_dict['seg_logits']\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    pred_probs = torch.softmax(logits, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define color map to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    ignore_index: [255, 255, 255],  # Silver\n",
    "    2: [0, 255, 0],  # \"Two-wheel Vehicle\"  # Lime\n",
    "    3: [255, 255, 0],  # \"Pedestrian\"  # Yellow\n",
    "    0: [0, 255, 255],  # \"Car\"  # Cyan\n",
    "    1: [255, 0, 0],  # \"Truck/Bus\"  # Red\n",
    "    7: [0, 0, 255],  # \"Traffic Light\"  # Blue\n",
    "    6: [0, 128, 128],  # \"Traffic Sign\"  # Teal\n",
    "    4: [128, 128, 128],  # \"Road\"  # Gray\n",
    "    5: [255, 0, 255],  # \"Sidewalk\"  # Magenta\n",
    "}\n",
    "# ignore_index = -1 일때만 작동\n",
    "color_map_lut_gt = np.array([color_map[i - 1] for i in range(len(color_map))]) / 255\n",
    "color_map_lut_pd = np.array([color_map[i] for i in range(len(color_map) - 1)]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = sample['coord']\n",
    "gt_color = color_map_lut_gt[sample['segment'] + 1]\n",
    "pd_color = color_map_lut_pd[pred_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_pcd = o3d.geometry.PointCloud()\n",
    "gt_pcd.points = o3d.utility.Vector3dVector(coord)\n",
    "gt_pcd.colors = o3d.utility.Vector3dVector(gt_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_pcd = o3d.geometry.PointCloud()\n",
    "pd_pcd.points = o3d.utility.Vector3dVector(coord)\n",
    "pd_pcd.colors = o3d.utility.Vector3dVector(pd_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론 데이터 시각화는 Jupyter 보다는 Visual Studio Code의 extension이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_plotly([gt_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_plotly([pd_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer all samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_ids = []\n",
    "all_pred_probs = []\n",
    "for i in range(len(data_list)):\n",
    "    data_dict = get_data(i)\n",
    "    data_dict = transform(data_dict)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_dict = model(data_dict)\n",
    "        logits = output_dict['seg_logits']\n",
    "\n",
    "        pred_ids = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        pred_probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    all_pred_ids.append(pred_ids)\n",
    "    all_pred_probs.append(pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save groundtruths and predictions in pcd format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_list)):\n",
    "    sample = get_data(i)\n",
    "    coord = sample['coord']\n",
    "    gt_color = color_map_lut_gt[sample['segment'] + 1]\n",
    "    pd_color = color_map_lut_pd[all_pred_ids[i]]\n",
    "    save_point_cloud(coord, gt_color, file_path=f'./exp/demo/groundtruth/gt{i:03d}.pcd')\n",
    "    save_point_cloud(coord, pd_color, file_path=f'./exp/demo/prediction/pd{i:03d}.pcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter unlabeled groundtruths/low confidence predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save only labeled groundtruths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_list)):\n",
    "    sample = get_data(i)\n",
    "    coord = sample['coord']\n",
    "    gt_color = color_map_lut_gt[sample['segment'] + 1]\n",
    "    \n",
    "    labeled_mask = sample['segment'] != -1\n",
    "    labeled_coord = coord[labeled_mask]\n",
    "    labeled_gt_color = gt_color[labeled_mask]\n",
    "    \n",
    "    save_point_cloud(labeled_coord, labeled_gt_color, file_path=f'./exp/demo/labeled_groundtruth/lgt{i:03d}.pcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save only confident predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_list)):\n",
    "    sample = get_data(i)\n",
    "    coord = sample['coord']\n",
    "    pd_color = color_map_lut_pd[all_pred_ids[i]]\n",
    "\n",
    "    confidence = np.max(all_pred_probs[i], axis=1)\n",
    "    confident_mask = confidence > 0.9\n",
    "    confident_coord = coord[confident_mask]\n",
    "    confident_pd_color = pd_color[confident_mask]\n",
    "    \n",
    "    save_point_cloud(confident_coord, confident_pd_color, file_path=f'./exp/demo/confident_prediction_90/cpd{i:03d}.pcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions for labeled groundtruths only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_list)):\n",
    "    sample = get_data(i)\n",
    "    coord = sample['coord']\n",
    "    pd_color = color_map_lut_pd[all_pred_ids[i]]\n",
    "    \n",
    "    labeled_mask = sample['segment'] != -1\n",
    "    labeled_coord = coord[labeled_mask]\n",
    "    labeled_pd_color = pd_color[labeled_mask]\n",
    "    \n",
    "    save_point_cloud(labeled_coord, labeled_pd_color, file_path=f'./exp/demo/labeled_prediction/lpd{i:03d}.pcd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
